# ReplicaSet(= 레플리카셋) 개념과 특징

레플리카셋은 파드 복제본을 생성하고 관리함.

* N개의 파드를 생성하기 위해 생성 명령을 N번 실행할 필요가 없음.
* 레플리카셋 오브젝트를 정의하고 원하는 Pod의 개수를 replicas 속성으로 선언.
* 클러스터 관리자 대신 파드 수가 부족하거나 넘치지 않게 파드 수를 조정.

# 왜 레플리카셋이 필요한가?

## 파드에 문제가 생겼다면...

파드는 즉시 종료되고 클라이언트 요청을 처리할 수 없음. Availability 확보 불가.  
쿠버네티스 클러스터 관리자가 항상 파드 상태를 감시하고 정상 복구해야 함 → 사실상 불가능
따라서 자동으로 N개의 파드를 필요 시 재시작하고 상태 이상에 대비해야 하는 수요가 있음.

"SW가 내결함성을 가짐" - fault tolerance

* 어떤 실패가 발생하더라도 SW는 동일한 기능을 수행할 수 있어야 함.
* SW가 내결함성이 없으면 고객 요구 사항을 만족시킬 수 없음.

## 레플리카셋이 이를 해결하는 방법

파드 혹은 노드 상태에 따라 파드의 수를 조정할 수 있도록 레플리카셋에게 역할을 위임.

* 레플리카셋 오브젝트를 이용해 파드 복제 / 복구 작업을 자동화.
* 쿠버네티스 클러스터 관리자는 레플리카셋 오브젝트를 만들어 필요한 파드의 수를 쿠버네티스에게 선언.
* 쿠버네티스는 레플리카셋 오브젝트의 요청서에 선언된 replicas 정보를 읽고 그 수 만큼 파드 실행을 보장.

# 레플리카셋 오브젝트 표현 방법

## 레플리카셋 오브젝트 yaml 규칙

```
apiVersion: apps/v1 # Kubernetes API 버전

kind: ReplicaSet # 오브젝트 타입

metadata: # 오브젝트를 유일하게 식별하기 위한 정보
  name: blue-app-rs # 오브젝트 이름
  labels: # 오브젝트 집합을 구할때 사용할 이름표, key-value 형태
    app: blue-app

spec: # 사용자가 원하는 파드의 바람직한 상태
  selector: # 레플리카셋이 관리해야 하는 파드를 선택하기 위한 label query
  replicas: # 실행할 파드 복제본 숫자 선언
  template: # Pod 실행 정보, Pod Template과동일(metadata, spec, …)
```

## 셀렉터 관련 규칙

```
spec:
  selector: # 레플리카셋이 관리할 파드 집합 선택
    matchLabels:
      app: blue-app # Pod label query 작성
  replicas: 3 # 유지할 파드 복제본 수
```

## 템플릿 관련 규칙

```
spec:
  selector:
    matchLabels:
      app: blue-app
  replicas: 3

  # 파드 템플릿 정의 여기서부터 시작, 파드 yaml 문법과 동일(단, appVersion이나 kind는 레플리카셋으로부터 가져와서 필요 없음)
  template: 
    metadata:
      labels:
        app: blue-app # 레플리카셋 셀렉터에서 정의한 레이블 포함 필수
    spec:
      containers:
      - name: blue-app
        image: blue-app:1.0
```

# 레플리카셋 주의할 점 & Tips

## 레플리카셋은 로드 밸런싱을 담당하지 않음!

포트 포워딩으로 호스트의 포트랑 레플리카셋 오브젝트의 포트랑 연결해도 항상 하나의 파드랑만 연결될 것.

## 레플리카셋 업데이트?

yaml 수정 후 `kubectl apply` 커맨드 다시 실행

## 실행 중인 파드의 수를 쉽게 늘리고 줄이기 가능

Up, Down이라 칭함.
레플리카셋에 선언한 replicas 값을 변경.  
레플리카셋이 현재 실행 중인 파드 수와 클러스터 관리자가 선언한 replicas 수를 비교해서 파드 숫자를 조정.

## 기존에 만들어둔 파드들도 나중에 만든 레플리카셋으로 관리가 가능

단, 기존에 있던 친구가 레플리카셋의 셀렉터 조건에 맞아야 함.  
레플리카셋을 생성할 때 셀렉터 조건에 맞는 파드가 이미 있다면 그거 빼고 나머지 숫자만 만듬.  
꼭 레플리카셋 생성 시 만들어진 친구들만 관리 대상인건 아님!!  

## 그럼 레이블이 다르던 파드를 관리하고 싶다면?

1. 기존 레플리카셋 삭제
2. 관리하고 싶은 파드에 관리할 파드들과 같은 레이블 추가
3. 추가한 레이블을 관리하는 새 레플리카셋 배포

## 배포한 레플리카셋의 파드 템플릿을 변경해도, 기존 파드에는 영향이 없음

레플리카셋에 선언한 replicas 수치가 변경되었을 경우에만 파드를 새로 생성하거나 제거함.  
대신, 어떤 파드가 죽었거나 해서 다시 켜지게 된다면 새로운 파드 템플릿 규칙에 따라 만들어지게 됨.  
따라서 일부러 끄지 않는 한, 구버전 템플릿 형태의 파드와 신버전 템플릿 형태의 파트가 공존함.

# 레플리카셋 롤백 전략

실행 중인 파드 장애 시 레플리카셋을 새로 생성하지 않고 이전 버전의 파드 배포가 가능.  

1. 레플리카셋 파드 템플릿의 이미지를 이전 버전으로 변경하고 적용.
2. 기존 파드 모두 제거. (replica 수치 0)
3. replica 수치 원복 적용.
4. 이전 버전의 이미지 베이스 파드들이 자동 배포.

혹은...

1. 레플리카셋 파드 템플릿의 이미지를 이전 버전으로 변경하고 적용.
2. 기존 파드의 레이블을 레플리카셋 셀렉터 조건과 다르게 일괄 변경.
3. 이전 버전의 이미지 베이스 파드들이 자동 배포.
4. 남아있는 문제 버전들 가지고 디버깅 진행 가능.

# 레플리카셋 포인트

* 레플리카셋으로 파드 복제본(replicas)를 한 번에 생성 & 관리.
    * 여러 노드에 걸쳐 배포된 파드의 Up/Down 상태를 감시하고 replicas 수 만큼 실행을 보장.
* 레플리카셋의 spec.selector.matchLabels 속성은 파드 템플릿의
spec.template.metadata.labels와 같아야 함.
* spec.replicas를 선언하지 않으면 기본값은 1.
* 레플리카셋이 우리 대신 파드 상태를 항상 감시.
* 파드 실행 중에도 replicas 조정이 자유로움.

# 레플리카셋 관련 명령어

* 레플리카셋 생성  
`$ kubectl apply -f <yaml 파일 경로>`
* 레플리카셋과 배포 이미지 확인  
`$ kubectl get rs <replicaset-name> -o wide`

* 레플리카셋의 이벤트 기록(파드 생성) 확인  
`$ kubectl describe rs <replicaset-name>`
* 레플리카셋의 파드 생성 이후 과정 확인  
`$ kubectl get events --sortby=.metadata.creationTimestamp`
* 레플리카셋 파드로 트래픽 전달  
`$ kubectl port-forward rs/<replicaset-name> 8080:8080`

* 레플리카셋 replicas 수 변경
`$ kubectl scale rs/<replicaset-name> --replicas <number of replicas>`

* 레플리카셋 삭제(산하 파드도 같이 삭제)  
`$ kubectl delete rs blue-replicaset`
* 레플리카셋 삭제(산하 파드 살려둠)  
`$ kubectl delete rs blue-replicaset --cascade=orphan`
* 레플리카셋 삭제(graceful)  
`$ kubectl scale rs blue-replicaset --replicas 0`  
`$ kubectl delete rs blue-replicaset`

# deployment(= 디플로이먼트) 개념과 특징

## 생각보다 귀찮은 레플리카셋 관리?

기존 레플리카셋의 도커 이미지가 업데이트 되었다고 가정할 때,  
우린 이걸 업데이트하기 위해, 아래와 같은 과정을 거침:  

1. 새로운 레플리카셋을 만들어 파드를 재배포하던지, 파드 템플릿을 바꾸고 적용
2. 필요없는 레플리카셋과 파드를 제거

롤백 / 새 버전 배포마다 이 과정을 반복.  
이 반복 작업을 대신해줄 뭔가가 없을까?  
*파드 템플릿 이미지가 바뀔 때마다
쿠버네티스가 알아서 레플리카셋을 생성하고 이전 파드를 제거해주면 안될까?* = 디플로이먼트!

## 파드 배포를 위한 3가지 정보

1. 셀렉터: 어떤 파드 집합을 대상으로 복사본을 관리?
2. replicas: 얼마나 파드를 생성?
3. 파드 템플릿 이미지: 파드에서 어떤 컨테이너를 실행?

## 디플로이먼트는 파드 배포 자동화를 위한 오브젝트! (레플리카셋 + 배포 전략)

* 새로운 파드를 롤아웃 / 롤백할 때 대신 레플리카셋을 생성(파드 복제).
* 다양한 배포 전략을 제공하고 이전 파드에서 새로운 파드로 전환 속도를 설정 가능.

이제는 파드를 배포할 때 레플리카셋이 아닌 디플로이먼트를 사용!

# 디플로이먼트 오브젝트 표현 방법

```
apiVersion: apps/v1 # Kubernetes API 버전

kind: Deployment # 오브젝트 타입

metadata: # 오브젝트를 유일하게 식별하기 위한 정보
  name: my-app # 오브젝트이름

spec: # 사용자가 원하는 파드의 바람직한 상태
  selector: # 레플리카셋을 통해 관리할 파드를 선택하기 위한 label query
    matchLabels:
      app: my-app
  replicas: 3 # 실행하고자 하는 파드 복제본 개수 선언
  template: # 파드 실행 정보 - 파드 템플릿과 동일 (metadata, spec, …)
    metadata:
      labels:
        app: my-app # 셀렉터에 정의한 레이블을 포함해야 함
    spec:
      containers:
      - name: my-app
        image: my-app:1.0
```

# 디플로이먼트 동작 컨셉: 롤아웃

디플로이먼트가 활성화 된 순간, 디플로이먼트는 `'레플리카셋_A'`를 만들어 yaml에 명세된 파드들을 생성하고 관리함.
이후 사용자가 이미 배포된 디플로이먼트에 파드 템플릿을 변경함.  
(유의미한 파드 템플릿 변경은 [여기](##-파드-배포를-위한-3가지-정보)를 참조.)  

## 배포 옵션 1: Recreate 배포

![](https://www.magalix.com/hs-fs/hubfs/Deployments%20101-2.jpg?width=3000&name=Deployments%20101-2.jpg)

변경 된 순간 디플로이먼트는 기존 `'레플리카셋_A'` 대신 `'레플리카셋_B'`를 생성하고, `'레플리카셋_B'`에서 새로운 파드 정보를 바탕으로 최신 사양의 파드 관리를 실행함.  
`'레플리카셋_A'`은 `'레플리카셋_B'`가 생성되는 순간 replicas 개수가 0개로 scale 조정이 들어가기 때문에, 파드를 전부 날림.  
때문에 순간적으로 서비스가 전체 종료되는 구간이 무조건 발생하며, 실제 라이브 서비스 운영 레벨에서는 보편적으로 쓰이지 않는 배포 방식.  
replicas 수 만큼의 컴퓨팅 리소스가 필요하며, 개발 단계에서는 자원 절약으로 인해 더 유리함.

## 배포 옵션 2: RollingUpdate 배포

![](https://www.magalix.com/hs-fs/hubfs/Deployments%20101-1.jpg?width=720&name=Deployments%20101-1.jpg)

변경 된 순간 디플로이먼트는 기존 `'레플리카셋_A'` 대신 `'레플리카셋_B'`를 생성하고, `'레플리카셋_B'`에서 새로운 파드 정보를 바탕으로 최신 사양의 파드 관리를 실행함.  
`'레플리카셋_A'`은 `'레플리카셋_B'`가 생성되는 시점부터 파드 갯수를 줄여나가는데,  
`'레플리카셋_B'`에서 파드가 하나 생성됨을 확인할 때 마다 `'레플리카셋_A'` 파드를 하나씩 날리는 방식.  
서비스가 일부 다른 버전으로 동작하지만, 라이브 서비스 운영 레벨에서 서비스가 완전히 중단되는 시점이 최소화되기 때문에 유리하지만,  
동시 실행되는 파드의 개수가 replicas를 넘는 시점이 있어 리소스가 더 많이 필요함.

### RollingUpdate 배포 속도 제어 옵션: maxUnavailable

롤링 업데이트 수행 중 유지하는 최소 파드의 비율(수)을 설정 가능.  
즉, 이전 버전의 파드를 전체 replicas 수의 최대 ??% 까지 즉시 삭제 가능!  
ex) replicas=10, maxUnavailable=30% 경우, 버전 변경 시 시작하자마자 3개 즉시 날리고 시작.

### RollingUpdate 배포 속도 제어 옵션: maxSurge

롤링 업데이트 수행 중 허용하는 최대 파드의 비율(수)을 설정 가능.  
즉, 신규 버전의 파드를 전체 replicas 수의 최대 ??% 까지 즉시 추가 가능!  
ex) replicas=10, maxSurge=30% 경우, 버전 변경 시 시작하자마자 3개 일단 만들고 시작.

# 디플로이먼트 동작 컨셉: 롤백

디플로이먼트는 롤아웃 히스토리를 Revision N으로 관리.  
ex) 최초 롤아웃은 Revision 1, 다음은 Revision 2...  
리비전 번호로 해당 배포의 상세 정보(파드 템플릿, 셀렉터 등)를 열람 가능.  

## 롤백 전략: Revision No. 롤백

`$ kubectl rollout undo deployment <deployment-name> --to-revision=1`

# 디플로이먼트 주의할 점 & Tips

## 디플로이먼트 내 오브젝트 명명 규칙

* 레플리카셋 이름  
`<deployment-name>-<pod-template-hash>`
* 파드 이름  
`<deployment-name>-<pod-template-hash>-<임의의문자열>`

## pod-template-hash?

파드 템플릿 데이터를 해싱해서 나온 string.  
디플로이먼트는 pod-template-hash 값을 레플리카셋의 레이블과 셀렉터, 파드의 레이블에 자동으로 추가.

## 명명 규칙으로 유추...

파드 템플릿이 변경되면 pod-template-hash 값도 바뀌기 때문에, 이름도 바뀌어야 함.  
따라서 파드 템플릿이 변경되면 새로운 레플리카셋을 만들어야 함.

## replicas 수만 바뀌었을 때?

새 레플리카셋이 만들어지지는 않고, 원래 레플리카셋의 replicas 옵션만 변경.  
이러면 기존 레플리카셋이 추가 파드만 만들고 끝남.  

## 디플로이먼트의 기본 배포 정책?

어떤 옵션 값도 주지 않았을 때, 디플로이먼트의 기본 배포 정책은 RollingUpdate 배포 방식을 채택함.  
.spec.strategy.type 형태로 yaml에 명세 가능(Recreate or RollingUpdate).  
maxUnavailable 수치와 maxSurge 수치는 아래 형태로 명세 가능:  
.spec.strategy.rollingUpdate.maxUnavailable  
.spec.strategy.rollingUpdate.maxSurge  

## 이전 레플리카셋이 남아있는데? 지워지는게 아니었나?

이전 레플리카셋은 디플로이먼트 내의 [리비전](#-디플로이먼트-동작-컨셉:-롤백) 기록으로써 남아있게 됨.  
따라서 안전한 운영을 위해서는 바로바로 지우는건 추천되지 않음.  
이전 기록을 몇 개 까지 저장할지는 yaml에 명세할 수 있음: .spec.revisionHistoryLimit 옵션! 참고로 기본 값은 10임.  

## 롤백 관련 코멘트도 적을 수 있음



# 디플로이먼트 관련 명령어

* 디플로이먼트 생성  
`$ kubectl apply -f <yaml 파일 경로>`
* 디플로이먼트 이벤트 확인  
`$ kubectl describe deployment <deployment-name>`
* 디플로이먼트 생성한 파드 상태 변화 확인  
`$ kubectl get deployment -w`
* 디플로이먼트 배포 상태 확인  
`$ kubectl rollout status deployment <deployment-name>`

* 디플로이먼트의 파드 replicas 변경  
`$ kubectl scale deployment <deployment-name> --replicas=<number-of-pod>`
* 디플로이먼트의 컨테이너 이미지 변경  
`$ kubectl set image deployment/<deployment-name> <container-name>=<image-name>`

* 리비전 조회  
`$ kubectl rollout history`
`$ kubectl rollout history deployment/my-app --revision=2`
* 리비전으로 롤백  
`$ kubectl rollout undo deployment/<deployment-name>`
`$ kubectl rollout undo deployment/<deployment-name> --to-revision=1`
* 배포 변경 사유 기록  
`$ kubectl annotate deployment/<deployment-name> kubernetes.io/change-cause=“some cause”`

* 레이블 셀렉터로 리소스 삭제  
`$ kubectl delete all -l <label-key>=<label-value>`